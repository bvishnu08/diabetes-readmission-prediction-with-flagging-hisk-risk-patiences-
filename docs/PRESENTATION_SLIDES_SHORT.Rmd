---
title: "Diabetes 30-Day Readmission Prediction"
subtitle: "High-Risk Patient Flagging - MSBA-265 Course Project"
author: "[Your Name]"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
    transition: faster
  beamer_presentation:
    theme: "metropolis"
    colortheme: "default"
    fonttheme: "default"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  fig.align = "center"
)

# Load libraries
library(ggplot2)
library(knitr)
```

## The Problem {.smaller}

**Why This Matters:**

- 30-day readmissions cost hospitals **billions** every year
- CMS penalizes hospitals with high readmission rates
- **Our Goal:** Predict which diabetic patients are at HIGH RISK before they leave the hospital
- **Why:** Early intervention = better outcomes = lower costs

**Bottom Line:** We built a tool that flags high-risk patients so hospitals can help them before it's too late.

---

## The Data {.smaller}

**What We Worked With:**

```{r data-overview, echo=FALSE, eval=FALSE}
# Data loading example
# File: src/preprocess.py

import pandas as pd
from src.config import Config

config = Config()
df = pd.read_csv(config.raw_data_path)
print(f"Dataset shape: {df.shape}")
print(f"Features: {df.shape[1] - 1}")
print(f"Patients: {df.shape[0]:,}")
```

- **Source:** UCI Machine Learning Repository
- **Size:** 101,766 patient encounters from 130 US hospitals
- **Features:** 50+ variables (age, medications, diagnoses, lab results, etc.)
- **Target:** Will patient be readmitted within 30 days? (Yes/No)

**What We Did:** Cleaned it up, handled missing values, and got it ready for modeling.

---

## The Challenges We Faced {.smaller}

**Real Problems, Real Solutions:**

### Challenge 1: Class Imbalance
- Most patients DON'T get readmitted (imbalanced data)
- **Problem:** Model might ignore the high-risk patients
- **Solution:** Tuned thresholds to prioritize RECALL (catch 80% of high-risk patients)

```{r threshold-tuning, echo=TRUE, eval=FALSE}
# Threshold tuning for 80% recall
# File: src/train.py

def tune_threshold_for_recall(y_true, y_proba, target_recall=0.80):
    from sklearn.metrics import recall_score
    
    thresholds = np.arange(0.1, 0.6, 0.01)
    best_threshold = 0.5
    best_recall = 0
    
    for threshold in thresholds:
        y_pred = (y_proba >= threshold).astype(int)
        recall = recall_score(y_true, y_pred)
        if recall >= target_recall and recall > best_recall:
            best_recall = recall
            best_threshold = threshold
    
    return best_threshold, best_recall
```

**Why:** Missing a high-risk patient is worse than a false alarm

---

## Challenges (Continued) {.smaller}

### Challenge 2: Too Many Features
- 50+ features = risk of overfitting
- **Solution:** Used Mutual Information to pick top 10 best features

```{r feature-selection, echo=TRUE, eval=FALSE}
# Feature selection using Mutual Information
# File: src/feature_selection.py

from sklearn.feature_selection import SelectKBest, mutual_info_classif

def select_features(X_train, y_train, k=10):
    selector = SelectKBest(
        score_func=mutual_info_classif,
        k=k
    )
    X_selected = selector.fit_transform(X_train, y_train)
    selected_features = X_train.columns[selector.get_support()]
    return X_selected, selected_features
```

**Why:** Simpler models are easier to understand and less likely to memorize data

---

## Challenges (Continued) {.smaller}

### Challenge 3: Missing Data
- Lots of `?` and `NULL` values
- **Solution:** Cleaned data, imputed missing values, dropped bad features

```{r preprocessing, echo=TRUE, eval=FALSE}
# Data preprocessing
# File: src/preprocess.py

def basic_clean(df, cfg):
    # Remove duplicates
    df = df.drop_duplicates()
    
    # Handle missing values
    df = df.replace('?', np.nan)
    df = df.dropna(thresh=len(df) * 0.5, axis=1)  # Drop columns with >50% missing
    
    # Impute remaining missing values
    from sklearn.impute import SimpleImputer
    imputer = SimpleImputer(strategy='most_frequent')
    df_imputed = imputer.fit_transform(df)
    
    return df_imputed
```

**Why:** Models need clean data to work properly

---

## Challenges (Continued) {.smaller}

### Challenge 4: Black Box Problem
- Complex models are hard to explain to doctors
- **Solution:** Built TWO models - one simple (Logistic Regression) and one powerful (XGBoost)

```{r two-models, echo=TRUE, eval=FALSE}
# Two-model approach
# File: src/model.py

from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

# Model 1: Interpretable
lr_model = LogisticRegression(
    max_iter=1000,
    random_state=42
)

# Model 2: Powerful
xgb_model = XGBClassifier(
    n_estimators=100,
    max_depth=5,
    random_state=42
)
```

**Why:** Doctors need to trust and understand the tool

---

## Our Pipeline {.smaller}

**How We Built It:**

```{r pipeline-flow, echo=FALSE, eval=FALSE}
# Pipeline flow (conceptual)
# Raw Data (101K patients)
#     ↓
# Preprocessing (Clean, Encode, Scale)
#     ↓
# Feature Selection (Pick Top 10)
#     ↓
# Train Models (Logistic Regression + XGBoost)
#     ↓
# Tune Thresholds (80% Recall Target)
#     ↓
# Evaluate on Test Data
#     ↓
# Dashboard (Interactive Tool)
```

**What We Did:** Built each step separately, made it modular, so anyone can run it end-to-end.

```{r run-pipeline, echo=TRUE, eval=FALSE}
# Run complete pipeline
# File: scripts/run_train.py

from src.train import train_models
from src.config import Config

config = Config()
train_models(config)  # Trains both models, saves to models/
```

---

## The Models We Built {.smaller}

**Two Models, Two Purposes:**

### Model 1: Logistic Regression
- **Why:** Simple, fast, easy to explain
- **Features:** Top 10 selected features
- **Best For:** When doctors need to understand WHY a patient is flagged

### Model 2: XGBoost
- **Why:** More powerful, finds complex patterns
- **Features:** All available features
- **Best For:** When accuracy is the priority

```{r model-comparison, echo=FALSE}
# Model comparison table
models <- data.frame(
  Model = c("Logistic Regression", "XGBoost"),
  Features = c("Top 10 selected", "All features"),
  Interpretability = c("High", "Low"),
  Performance = c("Good", "Better"),
  Use_Case = c("When explanation needed", "When accuracy priority")
)

kable(models, caption = "Model Comparison")
```

**Why Two?** Balance between "easy to explain" and "highly accurate" - use the right tool for the right situation.

---

## How We Measured Success {.smaller}

**The Metrics That Matter:**

```{r metrics-calculation, echo=TRUE, eval=FALSE}
# Evaluation metrics
# File: src/evaluate.py

from sklearn.metrics import (
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score
)

def evaluate_model(y_true, y_pred, y_proba):
    metrics = {
        'ROC-AUC': roc_auc_score(y_true, y_proba),
        'Precision': precision_score(y_true, y_pred),
        'Recall': recall_score(y_true, y_pred),  # MOST IMPORTANT!
        'F1-Score': f1_score(y_true, y_pred)
    }
    return metrics
```

**ROC-AUC:** Overall model performance (~0.65-0.72)  
**Precision:** How accurate are our high-risk flags? (~0.45-0.52)  
**F1-Score:** Balanced metric (~0.55-0.62)

**⭐ RECALL: THE MOST IMPORTANT (80%)**
- **What it means:** We catch 80% of all high-risk patients
- **Why it matters:** Missing a high-risk patient is WAY worse than a false alarm
- **We achieved our target!** ✅

---

## The Results {.smaller}

**How Well Did We Do?**

**Test Set:** 20,000 patients the models never saw before

```{r results-table, echo=FALSE}
# Results table
results <- data.frame(
  Metric = c("ROC-AUC", "Precision", "Recall", "F1-Score"),
  Logistic_Regression = c("0.65-0.70", "0.45-0.50", "80% ✅", "0.55-0.60"),
  XGBoost = c("0.68-0.72", "0.48-0.52", "80% ✅", "0.58-0.62")
)

kable(results, caption = "Model Performance Comparison")
```

**Key Findings:**
- ✅ Both models hit our 80% recall target
- ✅ XGBoost is slightly better overall
- ✅ Logistic Regression is easier to explain
- ⚠️ Some false alarms, but that's okay - we're catching the high-risk patients

**What This Means:** The models work! They can be used in real hospitals to help identify high-risk patients.

---

## What Features Matter Most? {.smaller}

**What Drives Readmission Risk?**

```{r feature-importance, echo=TRUE, eval=FALSE}
# Feature importance analysis
# File: src/model.py (Logistic Regression coefficients)

import numpy as np

# Get feature importance from trained model
lr_model = joblib.load('models/logreg_selected.joblib')
feature_importance = np.abs(lr_model.coef_[0])
top_features = np.argsort(feature_importance)[-10:][::-1]

print("Top 10 Features:")
for i, idx in enumerate(top_features, 1):
    print(f"{i}. {feature_names[idx]}: {feature_importance[idx]:.4f}")
```

**Top 5 Risk Factors:**
1. **Number of medications** - More meds = more complex = higher risk
2. **Number of diagnoses** - More conditions = sicker patient
3. **Time in hospital** - Longer stay = more serious case
4. **Number of lab procedures** - More tests = more complex
5. **Age group** - Older patients = higher risk

**Insight:** Medication complexity and clinical complexity are the biggest predictors.

---

## The Dashboard {.smaller}

**Making It Usable**

**What We Built:** Interactive web app using Streamlit

```{r dashboard-code, echo=TRUE, eval=FALSE}
# Dashboard main function
# File: dashboard.py

import streamlit as st
from src.clinical_utils import format_risk_level

def predict_risk(patient_data):
    # Load models
    lr_model = joblib.load('models/logreg_selected.joblib')
    xgb_model = joblib.load('models/xgb_selected.joblib')
    
    # Get predictions
    lr_prob = lr_model.predict_proba(patient_data)[0][1]
    xgb_prob = xgb_model.predict_proba(patient_data)[0][1]
    
    # Clinical interpretation
    threshold = 0.35  # Tuned for 80% recall
    risk_level = format_risk_level(lr_prob, threshold)
    
    return risk_level, lr_prob, xgb_prob
```

**Features:**
- **Real-time predictions** - Enter patient info, get instant risk score
- **Clinical interpretation** - Shows HIGH RISK or LOW RISK (not just numbers)
- **Model performance** - See how well the models are doing
- **Easy to use** - No coding required, doctors can use it right away

**How to Run:** `streamlit run dashboard.py`

---

## Challenges & Solutions Summary {.smaller}

**Problems We Solved:**

```{r challenges-table, echo=FALSE}
challenges <- data.frame(
  Challenge = c(
    "Class Imbalance",
    "Too Many Features",
    "Missing Data",
    "Black Box Problem"
  ),
  Solution = c(
    "Threshold tuning for 80% recall",
    "Feature selection (top 10)",
    "Data cleaning & imputation",
    "Two models + clinical categories"
  ),
  Result = c(
    "80% recall achieved",
    "Simpler, better models",
    "Reliable predictions",
    "Doctors can understand"
  )
)

kable(challenges, caption = "Challenges and Solutions")
```

**Takeaway:** Every problem had a solution, and we focused on what matters most - catching high-risk patients.

---

## What's Next? {.smaller}

**Future Plans:**

```{r future-plans, echo=FALSE, eval=FALSE}
# Future enhancements (conceptual)

future_work = [
    "1. Collect More Data - Better models need more data",
    "2. Better Models - Try neural networks, see if we can improve",
    "3. New Features - Add social factors, medication adherence",
    "4. Real Integration - Connect to hospital EMR systems",
    "5. Better Explainability - SHAP values per patient",
    "6. Real-World Testing - Validate in actual hospital setting"
]
```

1. **More Data** - Better models need more data
2. **Better Models** - Try neural networks, see if we can improve
3. **New Features** - Add social factors, medication adherence
4. **Real Integration** - Connect to hospital EMR systems
5. **Better Explainability** - SHAP values to show feature importance per patient
6. **Real-World Testing** - Validate in actual hospital setting

**Why:** Always room for improvement, and we want to make this actually useful in real hospitals.

---

## Key Takeaways {.smaller}

**What We Accomplished:**

```{r accomplishments, echo=FALSE}
accomplishments <- c(
  "✅ Built a complete pipeline - From raw data to predictions",
  "✅ Achieved 80% recall - We catch 80% of high-risk patients",
  "✅ Created a dashboard - Doctors can actually use it",
  "✅ Solved real challenges - Class imbalance, missing data, interpretability",
  "✅ Made it reproducible - Anyone can run it and get the same results"
)
```

✅ **Built a complete pipeline** - From raw data to predictions  
✅ **Achieved 80% recall** - We catch 80% of high-risk patients  
✅ **Created a dashboard** - Doctors can actually use it  
✅ **Solved real challenges** - Class imbalance, missing data, interpretability  
✅ **Made it reproducible** - Anyone can run it and get the same results

**Impact:**
- Helps reduce readmissions
- Improves patient care
- Saves hospitals money
- Supports clinical decisions

**Bottom Line:** We built something that actually works and can help real patients.

---

## Conclusion & Q&A {.smaller}

**Thank You!**

- **Problem:** Predict 30-day readmission risk
- **Solution:** Machine learning models (Logistic Regression + XGBoost)
- **Result:** 80% recall - successfully identify high-risk patients
- **Tool:** Interactive dashboard for clinicians

**Repository:** https://github.com/bvishnu08/diabetes-readmission-prediction-with-flagging-high-risk-patients-

**Questions?**

---

## Appendix: Key Code Files {.smaller}

```{r code-files, echo=FALSE, eval=FALSE}
# Project structure
code_files = {
    "Preprocessing": "src/preprocess.py",
    "Feature Selection": "src/feature_selection.py",
    "Model Training": "src/train.py",
    "Evaluation": "src/evaluate.py",
    "Dashboard": "dashboard.py",
    "Clinical Utils": "src/clinical_utils.py"
}
```

**Main Code Files:**
- `src/preprocess.py` - Data cleaning and preparation
- `src/feature_selection.py` - Feature selection using MI
- `src/train.py` - Model training pipeline
- `src/evaluate.py` - Model evaluation
- `dashboard.py` - Interactive Streamlit app
- `src/clinical_utils.py` - Risk interpretation

**Run Everything:**
```bash
python scripts/run_train.py    # Train models
python scripts/run_eval.py     # Evaluate models
streamlit run dashboard.py      # Launch dashboard
```

