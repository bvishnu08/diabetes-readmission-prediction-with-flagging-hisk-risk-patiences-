# Diabetes 30-Day Readmission Prediction

> **ðŸš€ EASIEST WAY: Download & Run Everything in One Command!**
> - **Mac/Linux:** `./download_and_run.sh` or `python download_and_run.py`
> - **Windows:** `download_and_run.bat` or `python download_and_run.py`
> 
> These scripts automatically download the repository from GitHub, set up the environment, install packages, train models, and evaluate them - **all in one command!**
> 
> **Alternative:** Manual setup â†’ **[SETUP_GUIDE.md](SETUP_GUIDE.md)** - Complete step-by-step instructions.

This MSBA-265 project predicts whether a diabetic inpatient will be
readmitted within 30 days of discharge. The repo is organized so it can be
explained live (P2) and submitted as a polished artifact (P3):
clean preprocessing, two complementary models, threshold tuning focused on high
recall, and a Streamlit dashboard that mirrors the CLI outputs.

## Project goal

- **Clinical motivation:** flag high-risk discharges early so care managers can
  intervene, improve outcomes, and avoid CMS HRRP penalties.
- **Dataset:** UCI *Diabetes 130-US Hospitals* (~100k encounters).
- **Target:** `readmitted_binary = 1` if the encounter is followed by another
  admission within 30 days.
- **Metric focus:** maximize recall for the positive class (do not miss true
  readmissions), then choose the operating point with the highest F1-score.

## Modeling approach

| Model | Role | Feature subset | Notes |
|-------|------|----------------|-------|
| Logistic Regression | Interpretable baseline for clinicians | Top 20 features (SelectKBest) | ColumnTransformer preprocessing, `class_weight="balanced"` |
| XGBoost | Deployment candidate | Top 25 features (SelectKBest) | 300 estimators, depth 4, learning-rate 0.05 |

Why two models?
1. **Logistic Regression** anchors the story around transparencyâ€”coefficients
   and odds ratios can be discussed with stakeholders.
2. **XGBoost** achieves stronger F1/ROC-AUC at the same high-recall target, so
   it is the recommended deployment model.

### Feature pool & selection

`Config.candidate_features` contains a curated set of 41 demographic, utilization,
and diabetes-therapy signals (age bucket, admission source, prior utilization,
insulin changes, diabetes medications, etc.). During training we run SelectKBest on this pool:
- LR keeps the top 20 features (easier to interpret while still capturing key signals).
- XGBoost keeps the top 25 (more expressive while still limited).

### Threshold tuning

After fitting each model we score the held-out test split, sweep thresholds from
0.05 â†’ 0.95, retain candidates with recall â‰¥ 0.65, and choose the threshold
with the highest F1-score for the readmission class. The final thresholds and
feature lists are persisted to `models/thresholds.json`, which keeps evaluation
and the dashboard perfectly in sync.

## Repository layout

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ dashboard.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ RUN_BOOK.md
â”‚   â”œâ”€â”€ COMPLETE_PROJECT_CODE.md
â”‚   â”œâ”€â”€ CODE_EXPLANATION.md
â”‚   â””â”€â”€ PIPELINE_DIFFERENCES.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ diabetic_data.csv
â”‚   â”‚   â””â”€â”€ IDS_mapping.csv
â”‚   â””â”€â”€ processed/{train,test}_processed.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ logreg_selected.joblib
â”‚   â”œâ”€â”€ xgb_selected.joblib
â”‚   â””â”€â”€ thresholds.json
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”œâ”€â”€ 02_modeling.ipynb
â”‚   â””â”€â”€ 03_implementation_details.ipynb
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ P2 Final_submission report.pdf
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_train.py
â”‚   â”œâ”€â”€ run_eval.py
â”‚   â””â”€â”€ run_dashboard.py
â””â”€â”€ src/
    â”œâ”€â”€ config.py
    â”œâ”€â”€ preprocess.py
    â”œâ”€â”€ feature_selection.py
    â”œâ”€â”€ model.py
    â”œâ”€â”€ train.py
    â””â”€â”€ evaluate.py
```

## How to run

```bash
python -m venv .venv
source .venv/bin/activate          # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# Train Logistic Regression + XGBoost and store thresholds
python scripts/run_train.py

# Evaluate on the held-out test split
python scripts/run_eval.py

# Optional dashboard (Streamlit)
streamlit run dashboard.py
```

Artifacts land in `models/` (joblib pipelines + thresholds) and
`data/processed/` (train/test CSVs) so notebooks, evaluation, and Streamlit all
consume the same outputs.

## Pipeline overview

1. **Preprocessing (`src/preprocess.py`)**
   - Strip whitespace, drop empty columns, replace `"?"` with `NaN`.
   - Map age buckets to ordinals and create `readmitted_binary`.
   - Restrict to the curated feature pool + target, stratified 80/20 split.
2. **Modeling (`src/model.py`, `src/train.py`)**
   - ColumnTransformer preprocessing, SelectKBest per model, fit LR + XGB.
3. **Threshold tuning (`src/train.py`)**
   - Enforce recall â‰¥ target (default 0.65) and pick the best F1 threshold.
4. **Evaluation (`src/evaluate.py`)**
   - Load processed test split and saved thresholds, print a side-by-side
     comparison with clinical interpretation (safe discharge vs. high-risk view),
     and recommend the stronger model.
5. **Dashboard (`dashboard.py`)**
   - Mirrors the CLI outputs: LR tab, XGB tab, ROC/confusion plots, and a
     prediction playground that uses the tuned thresholds.

## Interpretation & recommendation

Typical tuned metrics (vary slightly with the random split):

| Model | Threshold | Recall | Precision | F1 | ROC-AUC |
|-------|-----------|--------|-----------|----|---------|
| Logistic Regression (20 feats) | 0.45 | â‰ˆ0.70 | â‰ˆ0.15 | â‰ˆ0.24 | â‰ˆ0.64 |
| XGBoost (25 feats)             | 0.10 | â‰ˆ0.71 | â‰ˆ0.17 | â‰ˆ0.27 | â‰ˆ0.68 |

- **Logistic Regression** stays in the project as the interpretable, high-recall
  baseline that clinicians can interrogate.
- **XGBoost** is the recommended deployment model because it delivers the
  strongest F1 while maintaining recall at the target threshold.

## Future enhancements

- Add SHAP/feature-attribution notebooks for both models.
- Calibrate probabilities + quantify intervention cost/benefit.
- Extend the dashboard with fairness monitoring and cohort drill-downs.
# Diabetes 30-Day Readmission Prediction

Predict readmission risk for diabetic patients at discharge. The project keeps
the codebase classroom-ready (MSBA-265 P2/P3) while prioritizing **recall for
the positive class** so we catch as many high-risk patients as possible.

## Project Goal

**Clinical + business framing**
- Dataset: UCI *Diabetes 130-US Hospitals* (`data/raw/diabetic_data.csv`)
- Target: `readmitted_binary = 1` if the patient returns within 30 days
- Primary metric: Recall for class 1 (donâ€™t miss true readmissions)
- Secondary metrics: F1 (class 1) and ROC-AUC
- Use case: discharge screening â†’ false positives trigger extra outreach, but
  false negatives become penalties + readmissions.

## Feature pool & selection

We use `Config.candidate_features`, a curated pool of 41 features covering:

- **Demographics**: `age` (ordinal bucket), `gender`, `race`
- **Hospital visit context**: `admission_type_id`, `admission_source_id`, `discharge_disposition_id`
- **Utilization / severity**: `time_in_hospital`, `num_lab_procedures`,
  `num_procedures`, `num_medications`, `number_outpatient`,
  `number_emergency`, `number_inpatient`, `number_diagnoses`
- **Diabetes control & therapy**: `max_glu_serum`, `A1Cresult`, `change`, `insulin`,
  `diabetesMed`, and 23 diabetes medication indicators

During training, `SelectKBest` selects the most informative features from this pool:
- Logistic Regression uses the top 20 features (better interpretability)
- XGBoost uses the top 25 features (higher predictive power)

This feature selection strategy ensures both models focus on the most relevant signals
while maintaining interpretability for the logistic regression baseline.

## Models

| Model | Positioning | Details |
|-------|-------------|---------|
| Logistic Regression | **Baseline / interpretable** | ColumnTransformer preprocessing, balanced class weights, fast to explain to clinicians |
| XGBoost | **Advanced / higher recall** | 300 estimators, depth 4, learning-rate 0.05, subsample 0.8 |

Both pipelines use the same preprocessing (median imputation + scaling for
numerics, most-frequent + one-hot for categoricals).

### Threshold tuning strategy

1. Generate probabilities on the held-out test split.
2. Sweep thresholds from 0.05 â†’ 0.95.
3. Keep only thresholds where recall (class 1) â‰¥ **0.65**.
4. Among those, select the threshold with the highest **F1-score** for class 1.
5. Persist the chosen threshold + feature list to `models/thresholds.json`.

That gives us a clinically meaningful operating point and keeps evaluation,
plots, and the dashboard aligned.

## Pipeline Overview

1. **Preprocessing (`src/preprocess.py`)**
   - Strip whitespace, drop empty columns, replace `"?"` with `NaN`.
   - Convert `readmitted` â†’ `readmitted_binary` and map age buckets to integers.
   - Restrict columns to `candidate_features + target`.
   - Stratified 80/20 split saved to `data/processed/train_processed.csv`
     and `test_processed.csv`.

2. **Modeling (`src/model.py`, `src/train.py`)**
   - Shared preprocessing via ColumnTransformer.
   - Build + fit Logistic Regression and XGBoost pipelines.

3. **Threshold tuning (`src/train.py`)**
   - Apply the recall â‰¥ 0.65 rule, pick highest F1.
   - Persist thresholds + features to `models/thresholds.json`.

4. **Evaluation (`src/evaluate.py`)**
   - Load processed test split + saved models/thresholds.
   - Print a side-by-side comparison and recommend the stronger model.

5. **Dashboard (`dashboard.py`)**
   - Consumes the same artifacts to display metrics + interactive scoring.

## How to run

```bash
# 1. Install deps
pip install -r requirements.txt

# 2. Train both models + save artifacts
python scripts/run_train.py

# 3. Evaluate on held-out test split
python scripts/run_eval.py
```

Artifacts:
- `models/logreg_selected.joblib`
- `models/xgb_selected.joblib`
- `models/thresholds.json` (thresholds + feature list)
- `data/processed/train_processed.csv`, `test_processed.csv`

Optional dashboard:
```bash
streamlit run dashboard.py
# or python scripts/run_dashboard.py
```

## Project Structure

```
.
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ dashboard.py                 # Streamlit dashboard
â”‚
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ RUN_BOOK.md             # Step-by-step execution guide
â”‚   â”œâ”€â”€ COMPLETE_PROJECT_CODE.md # All project code in one file
â”‚   â”œâ”€â”€ CODE_EXPLANATION.md     # Deep dive into codebase
â”‚   â””â”€â”€ PIPELINE_DIFFERENCES.md # Notebook vs. production notes
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ diabetic_data.csv   # Raw UCI dataset
â”‚   â”‚   â””â”€â”€ IDS_mapping.csv     # Feature ID mappings
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ train_processed.csv # Processed training data
â”‚       â””â”€â”€ test_processed.csv # Processed test data
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ logreg_selected.joblib  # Logistic Regression model
â”‚   â”œâ”€â”€ xgb_selected.joblib     # XGBoost model
â”‚   â””â”€â”€ thresholds.json         # Stored thresholds + feature list
â”‚
â”œâ”€â”€ notebooks/                  # EDA and experiments
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”œâ”€â”€ 02_modeling.ipynb
â”‚   â””â”€â”€ 03_implementation_details.ipynb
â”‚
â”œâ”€â”€ reports/                    # Project reports and submissions
â”‚   â””â”€â”€ P2 Final_submission report.pdf
â”‚
â”œâ”€â”€ scripts/                    # CLI entrypoints
â”‚   â”œâ”€â”€ run_train.py           # Train both models
â”‚   â”œâ”€â”€ run_eval.py            # Evaluate both models
â”‚   â””â”€â”€ run_dashboard.py       # Launch dashboard
â”‚
â””â”€â”€ src/                        # Library code
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ config.py              # Paths, candidate features, thresholds
    â”œâ”€â”€ preprocess.py          # One source of truth for cleaning/splits
    â”œâ”€â”€ feature_selection.py   # SelectKBest feature selection
    â”œâ”€â”€ model.py               # Build LR + XGBoost pipelines
    â”œâ”€â”€ train.py               # Train + threshold tuning
    â”œâ”€â”€ evaluate.py            # Side-by-side comparison + clinical view
    â””â”€â”€ clinical_utils.py      # Clinical interpretation helpers
```

## Key Metrics Explained

- **Recall (Sensitivity)**: Proportion of actual readmissions correctly identified
  - **High recall** = Fewer missed high-risk patients (our primary goal)
- **Precision**: Proportion of predicted readmissions that actually occurred
  - Lower precision is acceptable if it means catching more true positives
- **F1-Score**: Harmonic mean of precision and recall
- **F2-Score**: Emphasizes recall over precision (beta=2)
  - Used for threshold tuning to prioritize catching readmissions
- **ROC-AUC**: Overall model discrimination ability
- **Accuracy**: Overall correctness (less important for imbalanced data)

## Reproducibility

- Fixed random seed (42) used throughout:
  - Train/test split
- Model training (Logistic Regression, XGBoost)
  - Feature selection
- All paths and configurations centralized in `src/config.py`
- Processed datasets saved to disk for consistency

## Notes

- **Curated features**: Reduces noise and keeps the narrative simple.
- **Class imbalance**: LR uses class weights, XGBoost uses scale_pos_weight=1
  but benefits from the tuned threshold.
- **Threshold tuning**: Done on the held-out split for clarity. In production,
  move this to a validation set or nested CV.
- **Interpretability**: Logistic Regression coefficients, plus XGBoost feature
  importances/SHAP (future enhancement).

## Future Enhancements

- SHAP/feature importance plots for both models.
- Calibrated probabilities + intervention cost modeling.
- Real-time scoring API + monitoring.
- Fairness analysis across demographics.

## License

This project is for educational purposes (MSBA-265 course project).
